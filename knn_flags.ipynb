{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: K-Nearest Neighbor (10 marks)\n",
    "\n",
    "Student Name:\n",
    "\n",
    "Student ID:\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: Friday, 18 March 2022 5pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count). Submissions more than 5 days late will not be accepted (resul in a mark of 0).\n",
    "<ul>\n",
    "    <li>one day late, -1.0;</li>\n",
    "    <li>two days late, -2.0;</li>\n",
    "    <li>three days late, -3.0;</li>\n",
    "    <li>four days late, -4.0;</li>\n",
    "    <li>five days late, -5.0;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Extensions</b>: Students who are demonstrably unable to submit a full solution in time due to medical reasons or other trauma, may apply for an extension.  In these cases, you should email <a href=\"mailto:hasti.samadi@unimelb.edu.au\">Hasti Samadi</a> as soon as possible after those circumstances arise. If you attend a GP or other health care service as a result of illness, be sure to provide a Health Professional Report (HPR) form (get it from the Special Consideration section of the Student Portal), you will need this form to be filled out if your illness develops into something that later requires a Special Consideration application to be lodged. You should scan the HPR form and send it with the extension requests.\n",
    "\n",
    "<b>Marks</b>: This assignment will be marked out of 10, and make up 10% of your overall mark for this subject.\n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page](https://canvas.lms.unimelb.edu.au/courses/124196/pages/python-and-jupyter-notebooks?module_item_id=3512182) on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn. You can use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. We reserve the right to deduct up to 2 marks for unreadable or exessively inefficient code.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board (Piazza -> Assignments -> A1); we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/124196/modules#module_662096\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>missing Authorship Declaration at the bottom of the page, -5.0\n",
    "<LI>incomplete or unsigned Authorship Declaration at the bottom of the page, -3.0\n",
    "</UL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this homework, you'll be applying the K-nearest neighbor (KNN) classification algorithm to a real-world machine learning data set. In particular, we will predict the primary color of national flags given a diverse set of features, including the country's size and population and other structural properties of the flag.\n",
    "\n",
    "Firstly, you will read in the dataset into a train and a test set, and you will create two feature sets (Q1). Secondly, you will implement different distance functions (Q2). Thirdly, you will implement two KNN classifiers (Q3, Q4) and apply it to the data set using different distance functions and parameter K (Q5). Finally, you will assess the quality of your classifier by comparing its class predictions to the true (or \"gold standard\") labels (Q6).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Loading the data (1.0 marks)\n",
    "\n",
    "**Instructions:** For this assignment we will develop a K-Nearest Neighbors (KNN) classifier to predict the predominant color of national flags. The list of classes (colors) is:\n",
    "\n",
    "```\n",
    "black\n",
    "blue\n",
    "brown\n",
    "gold\n",
    "green\n",
    "orange\n",
    "red\n",
    "white\n",
    "```\n",
    "\n",
    "We use a modified version of the Flags data set from the UCI Machine learning repository.\n",
    "\n",
    "The original data can be found here: https://archive.ics.uci.edu/ml/datasets/Flags.\n",
    "\n",
    "The dataset consists of 194 instances. Each instance corresponds to a national flag which has a unique identifier (itemX; first field) and is characterized with 25 features as described in the file *flags.names* which is provided as part of this assignment.\n",
    "\n",
    "You need to first obtain this dataset, which is on Canvas (assignment 1). The files *flags.features* and *flags.labels* contain the data we will use in this notebook. Make sure the files are saved in the same folder as this notebook. \n",
    "\n",
    "Both files are in comma-separated value (csv) format. The first line in each file is a header, naming each feature (or label).\n",
    "\n",
    "*flags.features* contains 194 instances, one line per instance. The first field is the unique instance identifier (name of country). The following fields contain the 25 features, as described in the file *flags.names*.\n",
    "\n",
    "*flags.labels* contains the true labels (i.e., one of the nine color classes above), one instance per line. Again, the first field is the instance identifier, and the second field the instance label.\n",
    "\n",
    "*flags.names* contains additional explanations about the data set and the features.\n",
    "\n",
    "All feature values are integers, and for Questions 1 through 5, we make the simplifying assumption that all values are indeed numeric. You may want to revisit this assumption in Question 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1a [0.5 mark]\n",
    "\n",
    "**Task**: Read the two files  \n",
    "1. create a **training_feature** set (list of features for the first 150 instances in the flags.* files) and a **training_label** set (list of labels for the corresponding). \n",
    "2. create a **test_feature** set (list of features of the remaining instances in the flags.* files) and a **test_label** set (list of labels for the corresponding). \n",
    "---------\n",
    "- Do **not** shuffle the data.\n",
    "- Do **not** modify feature or label representations.\n",
    "--------\n",
    "You may use any Python packages you want, but not change the specified data types (i.e., they should be of type List, and *not* dataframe, dictionary etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = open(\"flags.features\", 'r').readlines()\n",
    "labels = open(\"flags.labels\", 'r').readlines()\n",
    "\n",
    "train_features = []\n",
    "train_labels   = []\n",
    "test_features = []\n",
    "test_labels   = []\n",
    "\n",
    "\n",
    "###########################\n",
    "## YOUR CODE BEGINS HERE\n",
    "###########################\n",
    "\n",
    "train = data[1:]\n",
    "for i in range(0, 150):\n",
    "    line = train[i].strip().split(\",\")\n",
    "    \n",
    "    train_features.append(list(map(int, line[1:])))\n",
    "\n",
    "\n",
    "train = labels[1:]\n",
    "for i in range(0, 150):\n",
    "    train_labels.append(train[i].strip().split(\",\").pop())\n",
    "    \n",
    "    \n",
    "test = data[151:]\n",
    "for instance in test:\n",
    "    line = instance.strip().split(\",\")\n",
    "    test_features.append(list(map(int, line[1:])))\n",
    "\n",
    "test = labels[151:]\n",
    "for instance in test:\n",
    "    test_labels.append(instance.strip().split(\",\").pop())\n",
    "\n",
    "\n",
    "###########################\n",
    "## YOUR CODE ENDS HERE\n",
    "###########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1b [0.5 marks]\n",
    "\n",
    "**Task** Create a reduced feature set which only includes the \"Structural Flag Features\". The file *flag.names* specifies these features.\n",
    "\n",
    "----------\n",
    "\n",
    "You may use any Python packages you want, but not change the specified data types. You may (but don't have to) hard-code feature indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## YOUR CODE BEGINS HERE\n",
    "###########################\n",
    "\n",
    "df = pd.read_csv(\"flags.features\")\n",
    "test_features_structure = []\n",
    "train_features_structure = []\n",
    "\n",
    "df1 = df.iloc[0:150,6:]\n",
    "train_features_structure = df1.values.tolist()\n",
    "\n",
    "df2 = df.iloc[150:, 6:]\n",
    "test_features_structure = df2.values.tolist()\n",
    "\n",
    "###########################\n",
    "## YOUR CODE ENDS HERE\n",
    "###########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Distance Functions [1.0 mark]\n",
    "\n",
    "<b>Instructions</b>: Implement the two distance functions specified below. \n",
    "\n",
    "1. Manhattan distance\n",
    "2. Cosine distance\n",
    "\n",
    "Each distance function takes as input\n",
    "- Two feature vectors (each of type List)\n",
    "\n",
    "and returns as output\n",
    "- The distance between the two feature vectors (float)\n",
    "\n",
    "------------\n",
    "\n",
    "Use <b>only</b> the library imported below, i.e., <b>do not</b> use implementations from any other Python library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def manhattan_distance(fw1, fw2):\n",
    "    # insert code here\n",
    "    \n",
    "    distance = 0\n",
    "    for i in range(0, len(fw1)):\n",
    "        distance += abs(fw1[i] - fw2[i])\n",
    "    \n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cosine_distance(fw1, fw2):\n",
    "    # insert code here\n",
    "    \n",
    "    distance = 0\n",
    "    adotb = 0\n",
    "    a = 0\n",
    "    b = 0\n",
    "    for i in range(0, len(fw1)):\n",
    "        adotb += fw1[i] * fw2[i]\n",
    "        a += fw1[i] * fw1[i]\n",
    "        b += fw2[i] * fw2[i]\n",
    "    magA = math.sqrt(a)\n",
    "    magB = math.sqrt(b)\n",
    "    cosine = adotb / (magA * magB)\n",
    "    \n",
    "    distance = 1 - cosine\n",
    "    return distance\n",
    "\n",
    "#Testing data from lecture slide\n",
    "#a = [200, 300, 200]\n",
    "#b = [300, 200, 100]\n",
    "#c = [50, 40, 25]\n",
    "#m1 = [2, 1.4, 4.6, 5.5]\n",
    "#m2 = [1, 2.4, 6.6, 2.5]\n",
    "#print(cosine_distance(a, b))\n",
    "#print(manhattan_distance(m1, m2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: KNN Classifier [2.0 marks]\n",
    "\n",
    "<b>Instructions</b>: Here, you implement your KNN classifier. It takes as input \n",
    "- training data features\n",
    "- training data labels\n",
    "- test data features\n",
    "- parameter K\n",
    "- distance function(s) based on which nearest neighbors will be identified\n",
    "\n",
    "It returns as output \n",
    "- the predicted labels for the test data\n",
    "\n",
    "**Ties among distances**. If there are more than K instances with the same (smallest) distance value, consider the first K.\n",
    "\n",
    "**Ties at prediction time.** Ties can also occur at class prediction time when two (or more) classes are supported by the same number of neighbors. In that case choose the class of the 1 nearest neighbor.\n",
    "\n",
    "-----------\n",
    "\n",
    "**You should implement the classifier from scratch yourself**, i.e., <b> you must not</b> use an existing implementation in any Python library. You may use Python packages (e.g., math, numpy, collections, ...) to help with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(train_features, train_labels, test_features, k, dist_fun, weighted=False):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    ###########################\n",
    "    ## Your answer BEGINS HERE\n",
    "    ###########################\n",
    "    for test_instance in test_features:\n",
    "        distance_set = []\n",
    "        # calculate distance to each training data point in data set\n",
    "        for i in range(0, len(train_features)):\n",
    "            \n",
    "            distance = dist_fun(test_instance, train_features[i])\n",
    "            distance_set.append((distance, i))\n",
    "        ordered_distance = sorted(distance_set)\n",
    "        \n",
    "        # get k nearest neighbours\n",
    "        neighbours = []\n",
    "        for i in range(0, k):\n",
    "            neighbours.append(ordered_distance[i])\n",
    "       \n",
    "        # predict\n",
    "        \n",
    "        counts = {}\n",
    "        eps = 0.00001\n",
    "        nearest_distance = math.inf\n",
    "        nearest_index = 0\n",
    "        for (value, index) in neighbours:\n",
    "            if value < nearest_distance:\n",
    "                # get nearest neighbour in case of ties\n",
    "                nearest_distance = value\n",
    "                nearest_index = index\n",
    "            increment = 0\n",
    "            if weighted:\n",
    "                increment = 1 / (value + eps)\n",
    "            else:\n",
    "                increment = 1\n",
    "            # construct frequency table\n",
    "            if train_labels[index] in counts.keys():\n",
    "                \n",
    "                counts[train_labels[index]] += increment\n",
    "            else:\n",
    "                counts[train_labels[index]] = increment\n",
    "\n",
    "        # check for ties at prediction\n",
    "        tie = False\n",
    "        \n",
    "        \n",
    "            \n",
    "        candidate = max(counts.values())\n",
    "        \n",
    "        num = 0\n",
    "        selected = 0\n",
    "        for key in counts.keys():\n",
    "            if counts[key] == candidate:\n",
    "                num += 1\n",
    "                selected = key\n",
    "            if num > 1:\n",
    "                tie = True\n",
    "                break\n",
    "\n",
    "        if tie:\n",
    "            predictions.append(train_labels[nearest_index])\n",
    "            \n",
    "        else:\n",
    "            predictions.append(selected)\n",
    "                    \n",
    "        \n",
    "            \n",
    "    ###########################\n",
    "    ## Your answer ENDS HERE\n",
    "    ###########################\n",
    "   \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Weighted KNN Classifier [1.0 mark]\n",
    "\n",
    "<b>Instructions</b>: Extend your implementation of the KNN classifier in Question 3 to a Weighted KNN classifier. Use Inverse Distance as weights:\n",
    "\n",
    "$w_j=\\frac{1}{d_j+\\epsilon}$\n",
    "\n",
    "where\n",
    "\n",
    "- $d_j$ is the distance of of the jth nearest neighbor to the test instance\n",
    "- $\\epsilon=0.00001$\n",
    "\n",
    "Use the Boolean parameter `weighted` to specify the KNN version when calling the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Applying your KNN classifiers to the Flags Dataset [0.5 marks]\n",
    "\n",
    "**Using the functions you have implemented above, please**\n",
    "\n",
    "<b> 1. </b>\n",
    "For each of the distance functions you implemented in Question 2, construct (a) two majority voting KNN classifiers and (b) two weighted KNN classifiers, respectively, with \n",
    "\n",
    "- K=1\n",
    "- K=5\n",
    "\n",
    "You will obtain a total of 16 (2 distance functions x 2 K values x 2 KNN versions x 2 feature sets) classifiers.\n",
    "\n",
    "<b> 2. </b>\n",
    "Compute the test accuracy for each model, where the accuracy is the fraction of correctly predicted labels over all predictions. Use the `accuracy_score` function from the `sklearn.metrics` package to obtain your accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the *full* feature set\n",
      "\n",
      "manhattan (majority vote)\n",
      "K=1 0.273\n",
      "K=5 0.386\n",
      "-----------\n",
      "manhattan (weighted)\n",
      "K=1 0.273\n",
      "K=5 0.364\n",
      "\n",
      "cosine (majority vote)\n",
      "K=1 0.341\n",
      "K=5 0.364\n",
      "-----------\n",
      "cosine (weighted)\n",
      "K=1 0.341\n",
      "K=5 0.295\n",
      "\n",
      "\n",
      "Results on the *structure* feature set\n",
      "\n",
      "manhattan (majority vote)\n",
      "K=1 0.364\n",
      "K=5 0.409\n",
      "-----------\n",
      "manhattan (weighted)\n",
      "K=1 0.364\n",
      "K=5 0.432\n",
      "\n",
      "cosine (majority vote)\n",
      "K=1 0.386\n",
      "K=5 0.386\n",
      "-----------\n",
      "cosine (weighted)\n",
      "K=1 0.386\n",
      "K=5 0.386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "########################\n",
    "# Your code STARTS HERE\n",
    "########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accuracy_knn_man_1 = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 1, manhattan_distance))\n",
    "accuracy_knn_man_5 = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 5, manhattan_distance))\n",
    "\n",
    "accuracy_knn_man_1_structure = accuracy_score(\n",
    "    test_labels, KNN(train_features_structure, train_labels, test_features_structure, 1, manhattan_distance))\n",
    "\n",
    "accuracy_knn_man_5_structure = accuracy_score(\n",
    "    test_labels, KNN(train_features_structure, train_labels, test_features_structure, 5, manhattan_distance))\n",
    "\n",
    "accuracy_knn_man_1_w = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 1, manhattan_distance, \n",
    "                                                      weighted = True))\n",
    "accuracy_knn_man_5_w = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 5, manhattan_distance, \n",
    "                                                      weighted = True))\n",
    "\n",
    "accuracy_knn_man_1_w_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure,\n",
    "                                                                 1, manhattan_distance, weighted = True))\n",
    "accuracy_knn_man_5_w_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure,\n",
    "                                                                 5, manhattan_distance, weighted = True))\n",
    "accuracy_knn_cos_1 = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 1, cosine_distance))\n",
    "accuracy_knn_cos_5 = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 5, cosine_distance))\n",
    "\n",
    "accuracy_knn_cos_1_structure = accuracy_score(\n",
    "    test_labels, KNN(train_features_structure, train_labels, test_features_structure, 1, cosine_distance))\n",
    "accuracy_knn_cos_5_structure = accuracy_score(\n",
    "    test_labels, KNN(train_features_structure, train_labels, test_features_structure, 5, cosine_distance))\n",
    "\n",
    "accuracy_knn_cos_1_w = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 1, cosine_distance, \n",
    "                                                      weighted = True))\n",
    "accuracy_knn_cos_5_w = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 5, cosine_distance, \n",
    "                                                      weighted = True))\n",
    "\n",
    "accuracy_knn_cos_1_w_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure,\n",
    "                                                                 1, cosine_distance, weighted = True))\n",
    "accuracy_knn_cos_5_w_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure,\n",
    "                                                                 5, cosine_distance, weighted = True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "# Your code ENDS HERE\n",
    "########################\n",
    "\n",
    "\n",
    "\n",
    "print(\"Results on the *full* feature set\")\n",
    "\n",
    "print(\"\\nmanhattan (majority vote)\")\n",
    "print(\"K=1\", round(accuracy_knn_man_1, 3))\n",
    "print(\"K=5\", round(accuracy_knn_man_5, 3))\n",
    "\n",
    "print(\"-----------\\nmanhattan (weighted)\")\n",
    "print(\"K=1\", round(accuracy_knn_man_1_w, 3))\n",
    "print(\"K=5\", round(accuracy_knn_man_5_w, 3))\n",
    "\n",
    "print(\"\\ncosine (majority vote)\")\n",
    "print(\"K=1\", round(accuracy_knn_cos_1, 3))\n",
    "print(\"K=5\", round(accuracy_knn_cos_5, 3))\n",
    "\n",
    "print(\"-----------\\ncosine (weighted)\")\n",
    "print(\"K=1\", round(accuracy_knn_cos_1_w, 3))\n",
    "print(\"K=5\", round(accuracy_knn_cos_5_w, 3))\n",
    "\n",
    "print(\"\\n\\nResults on the *structure* feature set\")\n",
    "\n",
    "print(\"\\nmanhattan (majority vote)\")\n",
    "print(\"K=1\", round(accuracy_knn_man_1_structure, 3))\n",
    "print(\"K=5\", round(accuracy_knn_man_5_structure, 3))\n",
    "\n",
    "print(\"-----------\\nmanhattan (weighted)\")\n",
    "print(\"K=1\", round(accuracy_knn_man_1_w_structure, 3))\n",
    "print(\"K=5\", round(accuracy_knn_man_5_w_structure, 3))\n",
    "\n",
    "print(\"\\ncosine (majority vote)\")\n",
    "print(\"K=1\", round(accuracy_knn_cos_1_structure, 3))\n",
    "print(\"K=5\", round(accuracy_knn_cos_5_structure, 3))\n",
    "\n",
    "print(\"-----------\\ncosine (weighted)\")\n",
    "print(\"K=1\", round(accuracy_knn_cos_1_w_structure, 3))\n",
    "print(\"K=5\", round(accuracy_knn_cos_5_w_structure, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Analysis [4.5 marks]\n",
    "1. (a) Discuss the appropriateness of each of the distance functions to our two versions of the *flags* data set. Where appropriate, explain why you expect them to perform poorly referring to both their mathematical properties and the given feature set. **[0.5 marks]**\n",
    "\n",
    "    (b) Imagine you could choose a third distance function for the *Structure* feature set: either Hamming or Euclidean. Which one would you choose and why? Do you expect the results to be similar or different from Manhattan and Cosine [*N.B. you should only hypothesize based on the definitions of the metrics. You do not need to write any code*] **[0.5 marks]**\n",
    "    \n",
    "\n",
    "2. Does the Weighted KNN outperform the Majority voting version, or vice versa? Hypothesize why (not). **[1 mark]**\n",
    "\n",
    "\n",
    "3. (a) Plot a <a href=\"https://en.wikipedia.org/wiki/Histogram\">histogram</a> of the actual class frequencies in the test set, and a histogram of the predicted test labels for the knn_man_5 model. You should produce **a single plot** which shows the histogram both true and predicted labels. Label the x-axis and y-axis. [*N.B. you may use libraries like <a href=\"https://matplotlib.org/stable/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py\">matplotlib</a> or <a href=\"https://seaborn.pydata.org/introduction.html\">seaborne</a>*] **[1 mark]**\n",
    "\n",
    "    (b) Describe and explain the discrepancy between the true and predicted distributions. **[1 mark]**\n",
    "\n",
    "\n",
    "4. Do you think the accuracy is an appropriate evaluation metric for the *Flags* data set? Why (not)? **[0.5 marks]**\n",
    "\n",
    "<b>Each question should be answered in no more than 3-4 sentences.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) Mahattan distance is relatively appropriate since the data set consists high dimensionality of features. Cosine distance is less appropriate since Cosine distance measures the similarites between data points and forsakes the information conveyed through the magnitute of data. In this data set, majority of features are categorial or binary, whereas distance functions we adopted are more suitable for measureing numerical features, and cosine distance might not capture information conveyed by features such as landmass and population.\n",
    "\n",
    "1b) Hamming distance would be a beter choise over Euclidean distance. Since the data set consists a large number of features that are represented in a binary format, and Euclidean distance is not suitable for this data set due to its high dimentionaliy.  \n",
    "\n",
    "2) Weighted KNN should outperform the Majority voing version. Weighted KNN gives neighbours closer to the test instance more relavance to avoid the situation where the test instance is significantly close to a neighbour from one class but being outnumbered by other neighbours with farther distance from another class.\n",
    "\n",
    "*Type code for 3.(a) in the cell below, and answer 3.(b) below*\n",
    "\n",
    "3b) The prediction from KNN with k=5 and majority voting tends to prefer classes that contain a large number of instances in the data set. In this case, class \"red\" are the majority among the data set. With majority voting, it is expected that the KNN will choose the class with more neighbours for each test instance, therefore the \"red\" class stands out with most occurrence, causing the discrepancies. \n",
    "\n",
    "4) Accuracy works best when the all classes in the data set are balanced, that is all classes should share similiar percentage of amount in the data set. In this data set, classes are quite imbalanced since there are some colours significantly dominates other colours in terms of number of instances. Therefore, accuracy may not be an appropriate metric for this data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "################################################\n",
    "# Your answer to Question 6 (3) STARTS HERE\n",
    "################################################\n",
    "\n",
    "# get predicted labels\n",
    "predicted = KNN(train_features, train_labels, test_features, 5, manhattan_distance)\n",
    "\n",
    "# construct frequency table\n",
    "keys_dict_true = Counter(test_labels)\n",
    "keys_true = keys_dict_true.keys()\n",
    "keys_dict_predicted = Counter(predicted)\n",
    "keys_predicted = keys_dict_predicted.keys()\n",
    "\n",
    "# take the union of the labels that exist in both list\n",
    "all_color = list(set(keys_true) | set(keys_predicted))\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "\n",
    "# fill in the value for the labels\n",
    "for i in range(0, len(all_color)):\n",
    "    \n",
    "    if all_color[i] in keys_true:\n",
    "        actual_labels.append(keys_dict_true[all_color[i]])\n",
    "    else:\n",
    "        actual_labels.append(0)\n",
    "        \n",
    "    if all_color[i] in keys_predicted:\n",
    "        predicted_labels.append(keys_dict_predicted[all_color[i]])\n",
    "    else:\n",
    "        predicted_labels.append(0)\n",
    "\n",
    "# plot\n",
    "index = np.arange(len(all_color))\n",
    "plt.xticks(index, all_color)\n",
    "plt.bar(index - 0.2, predicted_labels, label = 'Predicted labels', width=0.4)\n",
    "plt.bar(index + 0.2, actual_labels, label = 'Actual labels', width = 0.4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "################################################\n",
    "# Your answer to Question 6 (3) ENDS HERE\n",
    "################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authorship Declaration</b>:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: JIACHEN PING\n",
    "   \n",
    "   <b>Dated</b>: 16/03/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
